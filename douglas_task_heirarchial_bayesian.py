# -*- coding: utf-8 -*-
"""Douglas_task_heirarchial_bayesian.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Q7gyCGo1gR383MiC5GSHLrWmrT7UdttG
"""

!pip install pystan==2.19.1.1

import pystan
import pandas as pd

data_df = pd.read_csv('merged_VTA_stan.csv')

# Count the number of subjects (N)
N = data_df['subjid'].nunique()

# Identify the maximum number of trials (T)
T = data_df.groupby('subjid').size().max()

# Determine the number of trials within each subject (Tsubj)
Tsubj = data_df.groupby('subjid').size().values

# Create placeholders with -1 (as per Stan data declaration)
cond = -1 * pd.DataFrame(index=range(N), columns=range(T)).fillna(-1).astype(int).values
decisions = -1 * pd.DataFrame(index=range(N), columns=range(T)).fillna(-1).astype(int).values
outcomes = -1 * pd.DataFrame(index=range(N), columns=range(T)).fillna(-1).astype(int).values
new_block = -1 * pd.DataFrame(index=range(N), columns=range(T)).fillna(-1).astype(int).values

# Populate the placeholders with actual data
for idx, subj_data in data_df.groupby('subjid'):
    trial_len = len(subj_data)
    cond[idx-1, :trial_len] = subj_data['cond'].values
    decisions[idx-1, :trial_len] = subj_data['decisions'].values
    outcomes[idx-1, :trial_len] = subj_data['outcomes'].values
    new_block[idx-1, :trial_len] = (subj_data['block'].diff() != 0).astype(int).values
    new_block[idx-1, 0] = 1  # First trial always starts a new block

stan_data = {
    'N': N,
    'T': T,
    'Tsubj': Tsubj,
    'cond': cond,
    'decisions': decisions,
    'outcomes': outcomes,
    'new_block': new_block
}

# Define your Stan model
stan_code = """
data {
  int<lower=1> N;                               // # of subjects
  int<lower=1> T;                               // max # of trials
  int<lower=1, upper=T> Tsubj[N];               // # of trials within subjects
  int<lower=-1, upper=3> cond[N, T];            // trial condition
  int<lower=-1, upper=2> decisions[N, T];       // decisions
  int<lower=-1, upper=1> outcomes[N, T];        // outcomes
  int<lower=-1, upper=1> new_block[N, T];       // new block indicator to reset Q-values
}

transformed data {
  // Define dimensions for initV
  int<lower=1> num_conditions = 3;
  int<lower=1> num_actions = 2;

  real initV[3, 2];
  for (i in 1:num_conditions) {
    for (j in 1:num_actions) {
      initV[i, j] = 0.0;
    }
  }
}

parameters {
  // Declare all parameters as vectors for vectorizing
  // Hyper(group)-parameters
  vector[6] mu_pr;
  vector<lower=0>[6] sigma;

  // Subject-level raw parameters (for Matt trick)
  vector[N] Arew_pr;
  vector[N] Apun_pr;
  vector[N] Aneu_pr;
  vector[N] Brew_pr;
  vector[N] Bpun_pr;
  vector[N] Bneu_pr;
}

transformed parameters {
  // Transform subject-level raw parameters
  vector<lower=0, upper=1>[N] Arew;
  vector<lower=0, upper=1>[N] Apun;
  vector<lower=0, upper=1>[N] Aneu;
  vector<lower=0>[N] Brew;
  vector<lower=0>[N] Bpun;
  vector<lower=0>[N] Bneu;

  for (i in 1:N) {
    Arew[i] = Phi_approx(mu_pr[1] + sigma[1] * Arew_pr[i]);
    Apun[i] = Phi_approx(mu_pr[2] + sigma[2] * Apun_pr[i]);
    Aneu[i] = Phi_approx(mu_pr[3] + sigma[3] * Aneu_pr[i]);
    Brew[i] = exp(mu_pr[4] + sigma[4] * Brew_pr[i]);
    Bpun[i] = exp(mu_pr[5] + sigma[5] * Bpun_pr[i]);
    Bneu[i] = exp(mu_pr[6] + sigma[6] * Bneu_pr[i]);
  }
}

model {
  // Hyperparameters
  mu_pr ~ normal(0, 1);
  sigma ~ normal(0, 0.2);

  // individual parameters
  Arew_pr  ~ normal(0, 1.0);
  Apun_pr  ~ normal(0, 1.0);
  Aneu_pr  ~ normal(0, 1.0);
  Brew_pr  ~ normal(0, 1.0);
  Bpun_pr  ~ normal(0, 1.0);
  Bneu_pr  ~ normal(0, 1.0);

  for (i in 1:N) {
    // Define values
    real Q[3, 2];     // expected value table
    real A;           // trial-level learning rate
    real B;           // trial-level sensitivity
    real PE;          // prediction error

    // Initialize values
    if (new_block[i, 1] == 1) {
      Q = initV;
    }

    // Which state/stimuli?
    if (cond[i, 1] == 1) {
      A = Arew[i];
      B = Brew[i];
    } else if (cond[i, 1] == 2) {
      A = Apun[i];
      B = Bpun[i];
    } else {
      A = Aneu[i];
      B = Bneu[i];
    }

    for (t in 1:Tsubj[i]) {
      // Initialize values
      if (new_block[i, t] == 1) {
        Q = initV;
      }

      // decisions
      decisions[i, t] ~ categorical(softmax(to_vector(Q[cond[i, t], ])));

      // Prediction error signals
      PE = B * outcomes[i, t] - Q[cond[i, t], decisions[i, t]];

      // Learning
      Q[cond[i, t], decisions[i, t]] += A * PE;
    }
  }
}

generated quantities {
  // For group level parameters
  real<lower=0, upper=1> mu_Arew;
  real<lower=0, upper=1> mu_Apun;
  real<lower=0, upper=1> mu_Aneu;
  real<lower=0> mu_Brew;
  real<lower=0> mu_Bpun;
  real<lower=0> mu_Bneu;

  // For log likelihood calculation
  real log_lik[N];

  // For posterior predictive check
  matrix[N, T] y_pred;

  // For prediction errors
  matrix[N, T] pred_error;

  mu_Arew = Phi_approx(mu_pr[1]);
  mu_Apun = Phi_approx(mu_pr[2]);
  mu_Aneu = Phi_approx(mu_pr[3]);
  mu_Brew = exp(mu_pr[4]);
  mu_Bpun = exp(mu_pr[5]);
  mu_Bneu = exp(mu_pr[6]);

  for (i in 1:N) {
    // Define values
    real Q[3, 2];     // expected value table
    real A;           // trial-level learning rate
    real B;           // trial-level sensitivity
    real PE;          // prediction error

    // Initialize values
    if (new_block[i, 1] == 1) {
      Q = initV;
    }

    // Which state/stimuli?
    if (cond[i, 1] == 1) {
      A = Arew[i];
      B = Brew[i];
    } else if (cond[i, 1] == 2) {
      A = Apun[i];
      B = Bpun[i];
    } else {
      A = Aneu[i];
      B = Bneu[i];
    }

    for (t in 1:Tsubj[i]) {
      // Initialize values
      if (new_block[i, t] == 1) {
        Q = initV;
      }

      // compute log likelihood of current trial
      log_lik[i] += categorical_lpmf(decisions[i, t] | softmax(to_vector(Q[cond[i, t], ])));

      // generate posterior prediction for current trial
      y_pred[i, t] = categorical_rng(softmax(to_vector(Q[cond[i, t],])));

      // Prediction error signals
      PE = B * outcomes[i, t] - Q[cond[i, t], decisions[i, t]];

      // Store prediction error
      pred_error[i, t] = PE;

      // Learning
      Q[cond[i, t], decisions[i, t]] += A * PE;
    }
  }
}

"""

# Compile the model
sm = pystan.StanModel(model_code=stan_code)

fit = sm.sampling(data=stan_data, iter=4000, warmup=2000, chains=4, seed=123, refresh=400)

fit_summary = fit.summary()
print(fit_summary)

rhats = fit_summary['summary'][:, -1]
if all(rhats < 1.05):
    print("All R-hat values are below 1.05, indicating potential convergence.")
else:
    print("Some R-hat values exceed 1.05, indicating potential lack of convergence.")

problematic_parameters = fit_summary['summary_rownames'][rhats > 1.05]
print("Parameters with potential convergence issues:", problematic_parameters)

!pip install arviz

import arviz as az

idata = az.from_pystan(fit)

az.plot_trace(idata)

print(fit)

fit_summary

samples = fit.extract()

Arew_samples = samples['Arew']

pred_errors = samples['pred_error']

pred_errors

Arew_samples

pip install numpy

import numpy as np  # Import the NumPy library

# Extract Tsubj values from data_df
Tsubj = data_df.groupby("subjid")["block"].max().values

# 1. Extract the prediction errors
pred_errors = fit['pred_error']

# 2. Calculate the mean prediction errors for each participant across all chains and iterations
mean_pred_errors = np.mean(pred_errors, axis=0)

# 3. Convert these mean prediction errors into a DataFrame
mean_pred_errors_df = pd.DataFrame(mean_pred_errors, columns=[f"Trial_{i+1}" for i in range(mean_pred_errors.shape[1])])

# 4. Display the first few rows of this DataFrame
print(mean_pred_errors_df.head())

# Save the DataFrame to a CSV file
mean_pred_errors_df.to_csv("mean_prediction_errors.csv", index=False)

# In Google Colab, you can then download the file with the following:
from google.colab import files
files.download("mean_prediction_errors.csv")

!pip install scipy

"""Extracting posterior using mode"""

from scipy.stats import mode

# Extract posterior decisions
posterior_decisions = fit.extract('y_pred')['y_pred']

# Calculate the mode across iterations for each trial and participant
mode_posterior_decisions = mode(posterior_decisions, axis=0)[0].squeeze()

import matplotlib.pyplot as plt  # Import the Matplotlib library

import seaborn as sns
from sklearn.metrics import mean_absolute_error

# Set seaborn style
sns.set_style("whitegrid")
sns.set_context("talk")

for subj in range(N):  # Assuming N is the number of subjects
    original_decisions = data_df[data_df['subjid'] == subj+1]['decisions'].values
    posterior_decisions = mode_posterior_decisions[subj, :len(original_decisions)]

    mae = mean_absolute_error(original_decisions, posterior_decisions)

    plt.figure(figsize=(12, 6))

    sns.scatterplot(x=range(len(original_decisions)), y=original_decisions, label='Original Decisions', marker='o', color='blue', s=60)
    sns.scatterplot(x=range(len(original_decisions)), y=posterior_decisions, label='Mode Posterior Decisions', marker='x', color='red', s=60)

    plt.title(f'Subject {subj+1} - Original vs Mode Posterior Decisions\nMAE = {mae:.3f}')
    plt.xlabel('Trials')
    plt.ylabel('Decisions')
    plt.legend(loc='upper right')
    plt.tight_layout()
    plt.show()

"""Randomly sample an iteration for each trial and participant"""

posterior_decisions = fit.extract('y_pred')['y_pred']
print(posterior_decisions.shape)

N, T = posterior_decisions.shape[1], posterior_decisions.shape[2]
sampled_posterior_decisions = np.zeros((N, T))

for i in range(N):
    for t in range(T):
        sampled_posterior_decisions[i, t] = np.random.choice(posterior_decisions[:, i, t])

import seaborn as sns
from sklearn.metrics import mean_absolute_error
import os

# Set seaborn style
sns.set_style("white")  # This will remove the grid lines
sns.set_context("talk")

jitter = 0.05  # This is the consistent jitter added to the posterior decisions

# Create a directory to store the plots
if not os.path.exists('plots'):
    os.makedirs('plots')

for subj in range(N):  # Assuming N is the number of subjects
    original_decisions = data_df[data_df['subjid'] == subj+1]['decisions'].values
    posterior_decisions = sampled_posterior_decisions[subj, :len(original_decisions)]

    posterior_jittered = posterior_decisions + jitter  # Add jitter to posterior decisions

    mae = mean_absolute_error(original_decisions, posterior_decisions)

    plt.figure(figsize=(12, 6))

    # Use muted color palette
    sns.scatterplot(x=range(len(original_decisions)), y=original_decisions, label='Original Decisions', marker='o', color=sns.color_palette("muted")[0], s=60)
    sns.scatterplot(x=range(len(original_decisions)), y=posterior_jittered, label='Sampled Posterior Decisions', marker='x', color=sns.color_palette("muted")[1], s=60)

    plt.title(f'Subject {subj+1} - Original vs Sampled Posterior Decisions\nMAE = {mae:.3f}', fontweight='bold')
    plt.xlabel('Trials')
    plt.ylabel('Decisions')

    # Place the legend at the middle of y-axis
    plt.legend(loc='center right', bbox_to_anchor=(1, 0.5))

    plt.tight_layout()

    # Save the figure
    plt.savefig(f'plots/subject_{subj+1}_plot.png', dpi=300)
    plt.show()